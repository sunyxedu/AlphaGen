{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "778f6498-857a-4fc5-857b-fbd653594be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None, 25, 2  0           []                               \n",
      "                                5, 1)]                                                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None, 25, 2  0           []                               \n",
      "                                5, 1)]                                                            \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, None, 625)    0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, None, 625)    0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, None, 1250)   0           ['reshape_3[0][0]',              \n",
      "                                                                  'reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, None, 64)     336640      ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, None, 64)     0           ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 50)     3250        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, None, 50)     0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, None, 2, 25)  0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, None, 2, 25)  0           ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 339,890\n",
      "Trainable params: 339,890\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Reshape, Activation, Dropout\n",
    "\n",
    "# Input\n",
    "input_1 = Input(shape=(None, 25, 25, 1))\n",
    "input_2 = Input(shape=(None, 25, 25, 1))\n",
    "\n",
    "# Reshape (25, 25, 1) to (25*25, 1)\n",
    "reshaped_input_1 = Reshape((-1, 25 * 25))(input_1)\n",
    "reshaped_input_2 = Reshape((-1, 25 * 25))(input_2)\n",
    "\n",
    "# Concatenate inputs along the last axis\n",
    "merged = tf.concat([reshaped_input_1, reshaped_input_2], axis=-1)  # Shape: (batch_size, time_steps, 25*25*2)\n",
    "\n",
    "# LSTM Layer without Dropout\n",
    "lstm_out = LSTM(units=64, return_sequences=True, recurrent_activation='sigmoid')(merged)\n",
    "\n",
    "# Dropout after LSTM\n",
    "lstm_out = Dropout(0.2)(lstm_out)\n",
    "\n",
    "# 50 Units: 2 Classification, Each 25 options\n",
    "dense_out = Dense(units=50)(lstm_out)\n",
    "\n",
    "# Dropout after Dense\n",
    "dense_out = Dropout(0.2)(dense_out)\n",
    "\n",
    "# Shape: (batch_size, time_steps, 2, 25)\n",
    "reshaped_out = Reshape((-1, 2, 25))(dense_out)\n",
    "\n",
    "# Apply softmax activation\n",
    "output = Activation('softmax')(reshaped_out)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "def cancel_loss(y_true, y_pred):\n",
    "    y_true_truncated = y_true[:, 40:, :, :]\n",
    "    y_pred_truncated = y_pred[:, 40:, :, :]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true_truncated, y_pred_truncated)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=cancel_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e74920b-3b4c-4889-adf2-4896e4739ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7688ac-8b1d-426c-9d0e-10cc0e0c7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(file_list, data_dir, batch_size):\n",
    "    total_files = len(file_list)\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(file_list)\n",
    "        for i in range(0, total_files, batch_size):\n",
    "            batch_files = file_list[i:i + batch_size]\n",
    "            feat_batch, army_batch, y_batch = [], [], []\n",
    "\n",
    "            for file in batch_files:\n",
    "                file_path = os.path.join(data_dir, file)\n",
    "                \n",
    "                data = np.load(file_path, allow_pickle=True)\n",
    "                \n",
    "                feat_batch.append(data['feat_padded'].astype(np.float32))\n",
    "                army_batch.append(data['army_padded'].astype(np.float32))\n",
    "                Y_padded = data['Y_padded'].astype(np.float32)\n",
    "                \n",
    "                # print(np.max(Y_padded[:,0]),np.max(Y_padded[:,1]))\n",
    "                y_dim1_one_hot = tf.keras.utils.to_categorical(Y_padded[:, 0], num_classes=25)\n",
    "                y_dim2_one_hot = tf.keras.utils.to_categorical(Y_padded[:, 1], num_classes=25)\n",
    "                y_one_hot = np.stack([y_dim1_one_hot, y_dim2_one_hot], axis=1)\n",
    "                y_batch.append(y_one_hot.astype(np.float32))\n",
    "            \n",
    "            feat_batch = np.array(feat_batch)\n",
    "            army_batch = np.array(army_batch)\n",
    "            y_batch = np.array(y_batch)\n",
    "            # print(feat_batch.shape)\n",
    "            yield (feat_batch, army_batch), y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0782cb3f-e9cc-41e1-b0a9-26212505bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dir = \"autodl-tmp/Numpy_Data\"\n",
    "file_names = os.listdir(data_dir)\n",
    "\n",
    "train_files, test_files = train_test_split(file_names, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)  # 75% train, 25% val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dfeb410-186b-4d79-9c9b-3f10fad97e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2615 872 872\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files),len(val_files),len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2077832b-790d-4d53-9162-0d0b5794fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feat_padded', 'army_padded', 'Y_padded']\n",
      "(1000, 2)\n",
      "Max value in Y_padded[:, 1]: 5\n"
     ]
    }
   ],
   "source": [
    "file_path = '/root/autodl-tmp/Numpy_Data/data_row_16.npz'\n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "data_keys = data.files\n",
    "print(data_keys)\n",
    "# print(data['Y_padded'].shape)\n",
    "Y_padded = data['Y_padded']\n",
    "print(Y_padded.shape)\n",
    "print(\"Max value in Y_padded[:, 1]:\", np.max(Y_padded[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b211c50a-6880-49ec-b199-fec44f9253a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "# val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "# test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9455cdfc-06ae-4e2a-9feb-460e0ad9350e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:36:42.747761: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 26 of 100\n",
      "2024-09-21 10:36:52.724021: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 53 of 100\n",
      "2024-09-21 10:37:02.677198: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 80 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_padded shape: (16, 1000, 25, 25)\n",
      "army_padded shape: (16, 1000, 25, 25)\n",
      "y_padded shape: (16, 1000, 2, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:37:10.259974: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset.take(1):  # Inspect 1 batch\n",
    "    (feat_padded, army_padded), y_padded = batch\n",
    "    print(\"feat_padded shape:\", feat_padded.shape)\n",
    "    print(\"army_padded shape:\", army_padded.shape)\n",
    "    print(\"y_padded shape:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f125af24-e50e-40b2-9525-0d0af9df8105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset element_spec=((TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32, name=None)), TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d304d1-cdaa-48c5-aee4-5704e791047e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:41:58.239016: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 25 of 100\n",
      "2024-09-21 10:42:08.173856: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 55 of 100\n",
      "2024-09-21 10:42:18.252031: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 85 of 100\n",
      "2024-09-21 10:42:23.306690: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n",
      "2024-09-21 10:42:24.149292: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 128s 553ms/step - loss: 1.2762 - val_loss: 0.7810\n",
      "Epoch 2/100\n",
      "163/163 [==============================] - 90s 553ms/step - loss: 1.0584 - val_loss: 0.7625\n",
      "Epoch 3/100\n",
      "163/163 [==============================] - 92s 564ms/step - loss: 1.0285 - val_loss: 0.7567\n",
      "Epoch 4/100\n",
      "163/163 [==============================] - 93s 571ms/step - loss: 0.9973 - val_loss: 0.7542\n",
      "Epoch 5/100\n",
      "163/163 [==============================] - 91s 556ms/step - loss: 0.9757 - val_loss: 0.7515\n",
      "Epoch 6/100\n",
      "163/163 [==============================] - 90s 551ms/step - loss: 0.9495 - val_loss: 0.7588\n",
      "Epoch 7/100\n",
      "163/163 [==============================] - 90s 550ms/step - loss: 0.9163 - val_loss: 0.7563\n",
      "Epoch 8/100\n",
      "163/163 [==============================] - 90s 553ms/step - loss: 0.9012 - val_loss: 0.7636\n",
      "Epoch 9/100\n",
      "163/163 [==============================] - 93s 572ms/step - loss: 0.8804 - val_loss: 0.7634\n",
      "Epoch 10/100\n",
      "163/163 [==============================] - 91s 560ms/step - loss: 0.8461 - val_loss: 0.7772\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, \n\u001b[1;32m      8\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m      9\u001b[0m           steps_per_epoch\u001b[38;5;241m=\u001b[39mtrain_steps, \n\u001b[1;32m     10\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     11\u001b[0m           validation_steps\u001b[38;5;241m=\u001b[39mvalidation_steps,\n\u001b[1;32m     12\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_full.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mtest_generator\u001b[49m, steps\u001b[38;5;241m=\u001b[39mtest_steps)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_generator' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "total_files = len(os.listdir(\"autodl-tmp/Numpy_Data\"))\n",
    "train_steps = len(train_files) // batch_size\n",
    "validation_steps = len(val_files) // batch_size\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit(train_dataset, \n",
    "          epochs=100, \n",
    "          steps_per_epoch=train_steps, \n",
    "          validation_data=val_dataset,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "model.save('model_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6094742d-1d4a-45ba-af0a-bfa52115ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 22s 402ms/step - loss: 0.7873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7872810363769531"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_steps = len(test_files) // batch_size\n",
    "model.evaluate(test_dataset, steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d838f7d-d144-4a09-a366-d6c2c29fac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(\n",
    "    feat_padded, army_padded, Y_padded, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fb420-00e1-4a8f-8b42-456aa19b2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "del feat_padded, army_padded, Y_padded\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a53f2a-e7a2-4f22-8cc7-5ab228a363f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.squeeze(y_train, axis=2)\n",
    "y_test = np.squeeze(y_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90137d9-b0c7-4ecd-9b93-13d9fe62070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 25\n",
    "\n",
    "y_train_class_1 = tf.one_hot(y_train[..., 0], depth=num_classes)\n",
    "y_train_class_2 = tf.one_hot(y_train[..., 1], depth=num_classes)\n",
    "y_train_categorical = tf.stack([y_train_class_1, y_train_class_2], axis=-2)\n",
    "\n",
    "y_test_class_1 = tf.one_hot(y_test[..., 0], depth=num_classes)\n",
    "y_test_class_2 = tf.one_hot(y_test[..., 1], depth=num_classes)\n",
    "y_test_categorical = tf.stack([y_test_class_1, y_test_class_2], axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1235c-b10e-42d5-a90d-5f2d8daaaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit([X1_train, X2_train], y_train_categorical, epochs=100, batch_size=16, verbose=1)\n",
    "model.save('model_full.h5')\n",
    "model.evaluate([X1_test, X2_test], y_test_categorical, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
