{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391e5377-e146-4b57-a138-d97e95e4945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 12:03:27.254150: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778f6498-857a-4fc5-857b-fbd653594be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 12:03:28.949986: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-21 12:03:29.629269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22134 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:e3:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 25, 2  0           []                               \n",
      "                                5, 1)]                                                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 25, 2  0           []                               \n",
      "                                5, 1)]                                                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, None, 625)    0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, None, 625)    0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, None, 1250)   0           ['reshape[0][0]',                \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, None, 256)    1543168     ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, None, 256)    525312      ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 100)    25700       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 50)     5050        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, None, 50)     0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, None, 2, 25)  0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, None, 2, 25)  0           ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,099,230\n",
      "Trainable params: 2,099,230\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Reshape, Activation, Dropout\n",
    "\n",
    "# Input\n",
    "input_1 = Input(shape=(None, 25, 25, 1))\n",
    "input_2 = Input(shape=(None, 25, 25, 1))\n",
    "\n",
    "# Reshape (25, 25, 1) to (25*25, 1)\n",
    "reshaped_input_1 = Reshape((-1, 25 * 25))(input_1)\n",
    "reshaped_input_2 = Reshape((-1, 25 * 25))(input_2)\n",
    "\n",
    "# Concatenate inputs along the last axis\n",
    "merged = tf.concat([reshaped_input_1, reshaped_input_2], axis=-1)  # Shape: (batch_size, time_steps, 25*25*2)\n",
    "\n",
    "# LSTM Layer without Dropout\n",
    "lstm_out = LSTM(units=256, return_sequences=True, recurrent_activation='sigmoid')(merged)\n",
    "lstm_out = LSTM(units=256, return_sequences=True, recurrent_activation='sigmoid')(lstm_out)\n",
    "\n",
    "# Dropout after LSTM\n",
    "lstm_out = Dropout(0.2)(lstm_out)\n",
    "\n",
    "# 50 Units: 2 Classification, Each 25 options\n",
    "dense_out = Dense(units=100)(lstm_out)\n",
    "dense_out = Dense(units=50)(dense_out)\n",
    "\n",
    "# Dropout after Dense\n",
    "dense_out = Dropout(0.2)(dense_out)\n",
    "\n",
    "# Shape: (batch_size, time_steps, 2, 25)\n",
    "reshaped_out = Reshape((-1, 2, 25))(dense_out)\n",
    "\n",
    "# Apply softmax activation\n",
    "output = Activation('softmax')(reshaped_out)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "def cancel_loss(y_true, y_pred):\n",
    "    y_true_truncated = y_true[:, 40:, :, :]\n",
    "    y_pred_truncated = y_pred[:, 40:, :, :]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true_truncated, y_pred_truncated)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=cancel_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e74920b-3b4c-4889-adf2-4896e4739ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d7688ac-8b1d-426c-9d0e-10cc0e0c7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(file_list, data_dir, batch_size):\n",
    "    total_files = len(file_list)\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(file_list)\n",
    "        for i in range(0, total_files, batch_size):\n",
    "            batch_files = file_list[i:i + batch_size]\n",
    "            feat_batch, army_batch, y_batch = [], [], []\n",
    "\n",
    "            for file in batch_files:\n",
    "                file_path = os.path.join(data_dir, file)\n",
    "                \n",
    "                data = np.load(file_path, allow_pickle=True)\n",
    "                \n",
    "                feat_batch.append(data['feat_padded'].astype(np.float32))\n",
    "                army_batch.append(data['army_padded'].astype(np.float32))\n",
    "                Y_padded = data['Y_padded'].astype(np.float32)\n",
    "                \n",
    "                # print(np.max(Y_padded[:,0]),np.max(Y_padded[:,1]))\n",
    "                y_dim1_one_hot = tf.keras.utils.to_categorical(Y_padded[:, 0], num_classes=25)\n",
    "                y_dim2_one_hot = tf.keras.utils.to_categorical(Y_padded[:, 1], num_classes=25)\n",
    "                y_one_hot = np.stack([y_dim1_one_hot, y_dim2_one_hot], axis=1)\n",
    "                y_batch.append(y_one_hot.astype(np.float32))\n",
    "            \n",
    "            feat_batch = np.array(feat_batch)\n",
    "            army_batch = np.array(army_batch)\n",
    "            y_batch = np.array(y_batch)\n",
    "            # print(feat_batch.shape)\n",
    "            yield (feat_batch, army_batch), y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0782cb3f-e9cc-41e1-b0a9-26212505bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dir = \"autodl-tmp/Numpy_Data\"\n",
    "file_names = os.listdir(data_dir)\n",
    "\n",
    "train_files, test_files = train_test_split(file_names, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42)  # 75% train, 25% val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dfeb410-186b-4d79-9c9b-3f10fad97e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2615 872 872\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files),len(val_files),len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2077832b-790d-4d53-9162-0d0b5794fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feat_padded', 'army_padded', 'Y_padded']\n",
      "(1000, 2)\n",
      "Max value in Y_padded[:, 1]: 5\n"
     ]
    }
   ],
   "source": [
    "file_path = '/root/autodl-tmp/Numpy_Data/data_row_16.npz'\n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "data_keys = data.files\n",
    "print(data_keys)\n",
    "# print(data['Y_padded'].shape)\n",
    "Y_padded = data['Y_padded']\n",
    "print(Y_padded.shape)\n",
    "print(\"Max value in Y_padded[:, 1]:\", np.max(Y_padded[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b211c50a-6880-49ec-b199-fec44f9253a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "# val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "# test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9455cdfc-06ae-4e2a-9feb-460e0ad9350e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 12:03:42.975300: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 23 of 100\n",
      "2024-09-21 12:03:52.970563: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 48 of 100\n",
      "2024-09-21 12:04:02.810704: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 71 of 100\n",
      "2024-09-21 12:04:12.856307: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 95 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_padded shape: (16, 1000, 25, 25)\n",
      "army_padded shape: (16, 1000, 25, 25)\n",
      "y_padded shape: (16, 1000, 2, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 12:04:14.755920: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "# for batch in train_dataset.take(1):  # Inspect 1 batch\n",
    "#     (feat_padded, army_padded), y_padded = batch\n",
    "#     print(\"feat_padded shape:\", feat_padded.shape)\n",
    "#     print(\"army_padded shape:\", army_padded.shape)\n",
    "#     print(\"y_padded shape:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f125af24-e50e-40b2-9525-0d0af9df8105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset element_spec=((TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32, name=None)), TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d304d1-cdaa-48c5-aee4-5704e791047e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 12:04:27.941161: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 24 of 100\n",
      "2024-09-21 12:04:37.905893: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 52 of 100\n",
      "2024-09-21 12:04:47.839490: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 80 of 100\n",
      "2024-09-21 12:04:56.170880: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n",
      "2024-09-21 12:04:57.030745: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n",
      "2024-09-21 12:04:58.041574: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 159s 712ms/step - loss: 1.0745 - val_loss: 0.7581\n",
      "Epoch 2/100\n",
      "163/163 [==============================] - 116s 715ms/step - loss: 1.0232 - val_loss: 0.7469\n",
      "Epoch 3/100\n",
      "163/163 [==============================] - 116s 713ms/step - loss: 0.9717 - val_loss: 0.7605\n",
      "Epoch 4/100\n",
      "163/163 [==============================] - 114s 702ms/step - loss: 0.9609 - val_loss: 0.7652\n",
      "Epoch 5/100\n",
      "163/163 [==============================] - 115s 704ms/step - loss: 0.9177 - val_loss: 0.7790\n",
      "Epoch 6/100\n",
      "163/163 [==============================] - 116s 714ms/step - loss: 0.8685 - val_loss: 0.7954\n",
      "Epoch 7/100\n",
      "163/163 [==============================] - 117s 717ms/step - loss: 0.8301 - val_loss: 0.8111\n",
      "Epoch 8/100\n",
      "163/163 [==============================] - 115s 707ms/step - loss: 0.7766 - val_loss: 0.8540\n",
      "Epoch 9/100\n",
      "163/163 [==============================] - 114s 699ms/step - loss: 0.7178 - val_loss: 0.9103\n",
      "Epoch 10/100\n",
      "163/163 [==============================] - 115s 706ms/step - loss: 0.6656 - val_loss: 0.9410\n",
      "Epoch 11/100\n",
      "163/163 [==============================] - 115s 709ms/step - loss: 0.6225 - val_loss: 0.9726\n",
      "Epoch 12/100\n",
      "163/163 [==============================] - 115s 708ms/step - loss: 0.6009 - val_loss: 1.0439\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "total_files = len(os.listdir(\"autodl-tmp/Numpy_Data\"))\n",
    "train_steps = len(train_files) // batch_size\n",
    "validation_steps = len(val_files) // batch_size\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model.fit(train_dataset, \n",
    "          epochs=100, \n",
    "          steps_per_epoch=train_steps, \n",
    "          validation_data=val_dataset,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "model.save('model_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6094742d-1d4a-45ba-af0a-bfa52115ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 26s 488ms/step - loss: 0.7905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7904868721961975"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_steps = len(test_files) // batch_size\n",
    "model.evaluate(test_dataset, steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d838f7d-d144-4a09-a366-d6c2c29fac95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [16, 16, 1000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m X1_train, X1_test, X2_train, X2_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeat_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marmy_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2646\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2650\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2651\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:453\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 453\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [16, 16, 1000]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(\n",
    "    feat_padded, army_padded, Y_padded, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fb420-00e1-4a8f-8b42-456aa19b2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "del feat_padded, army_padded, Y_padded\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a53f2a-e7a2-4f22-8cc7-5ab228a363f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.squeeze(y_train, axis=2)\n",
    "y_test = np.squeeze(y_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90137d9-b0c7-4ecd-9b93-13d9fe62070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 25\n",
    "\n",
    "y_train_class_1 = tf.one_hot(y_train[..., 0], depth=num_classes)\n",
    "y_train_class_2 = tf.one_hot(y_train[..., 1], depth=num_classes)\n",
    "y_train_categorical = tf.stack([y_train_class_1, y_train_class_2], axis=-2)\n",
    "\n",
    "y_test_class_1 = tf.one_hot(y_test[..., 0], depth=num_classes)\n",
    "y_test_class_2 = tf.one_hot(y_test[..., 1], depth=num_classes)\n",
    "y_test_categorical = tf.stack([y_test_class_1, y_test_class_2], axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1235c-b10e-42d5-a90d-5f2d8daaaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit([X1_train, X2_train], y_train_categorical, epochs=100, batch_size=16, verbose=1)\n",
    "model.save('model_full.h5')\n",
    "model.evaluate([X1_test, X2_test], y_test_categorical, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
