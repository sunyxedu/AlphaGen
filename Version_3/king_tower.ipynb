{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391e5377-e146-4b57-a138-d97e95e4945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 17:09:45.268406: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "778f6498-857a-4fc5-857b-fbd653594be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_38 (InputLayer)          [(None, None, 25, 2  0           []                               \n",
      "                                5, 1)]                                                            \n",
      "                                                                                                  \n",
      " input_39 (InputLayer)          [(None, None, 25, 2  0           []                               \n",
      "                                5, 1)]                                                            \n",
      "                                                                                                  \n",
      " input_40 (InputLayer)          [(None, None, 25, 2  0           []                               \n",
      "                                5, 1)]                                                            \n",
      "                                                                                                  \n",
      " time_distributed_13 (TimeDistr  (None, None, 25, 64  533952     ['input_38[0][0]']               \n",
      " ibuted)                        )                                                                 \n",
      "                                                                                                  \n",
      " time_distributed_14 (TimeDistr  (None, None, 25, 64  533952     ['input_39[0][0]']               \n",
      " ibuted)                        )                                                                 \n",
      "                                                                                                  \n",
      " time_distributed_15 (TimeDistr  (None, None, 25, 64  533952     ['input_40[0][0]']               \n",
      " ibuted)                        )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_3 (TFOpLam  (None, None, 64)    0           ['time_distributed_13[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_4 (TFOpLam  (None, None, 64)    0           ['time_distributed_14[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_5 (TFOpLam  (None, None, 64)    0           ['time_distributed_15[0][0]']    \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, None, 192)    0           ['tf.math.reduce_mean_3[0][0]',  \n",
      "                                                                  'tf.math.reduce_mean_4[0][0]',  \n",
      "                                                                  'tf.math.reduce_mean_5[0][0]']  \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, None, 192)    0           ['tf.concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, None, 256)    459776      ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, None, 256)    525312      ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, None, 256)    0           ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, None, 100)    25700       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, None, 625)    63125       ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, None, 625)    0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, None, 625)    0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,607,865\n",
      "Trainable params: 1,607,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Reshape, Activation, Dropout, GlobalAveragePooling1D, TimeDistributed\n",
    "\n",
    "class VisionTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embed_dim, num_transformer_blocks, patch_size, num_patches):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.patch_dim = patch_size * patch_size\n",
    "\n",
    "        self.patch_embedding = layers.Dense(embed_dim)  # Linear projection for patches\n",
    "        self.pos_embedding = tf.Variable(tf.random.normal([1, num_patches, embed_dim]))  # Positional encoding\n",
    "        self.transformer_blocks = [\n",
    "            tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)\n",
    "            for _ in range(self.num_transformer_blocks)\n",
    "        ]\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=x,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "        patch_dims = self.patch_size * self.patch_size\n",
    "        x = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        x = self.patch_embedding(x) + self.pos_embedding\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, x)\n",
    "        return x\n",
    "\n",
    "input_1 = Input(shape=(None, 25, 25, 1))\n",
    "input_2 = Input(shape=(None, 25, 25, 1))\n",
    "input_3 = Input(shape=(None, 25, 25, 1))\n",
    "\n",
    "patch_size = 5\n",
    "num_patches = (25 // patch_size) ** 2\n",
    "\n",
    "vit_layer = VisionTransformer(num_heads=8, embed_dim=64, num_transformer_blocks=4, patch_size=patch_size, num_patches=num_patches)\n",
    "\n",
    "vit_output_1 = TimeDistributed(vit_layer)(input_1)\n",
    "vit_output_2 = TimeDistributed(vit_layer)(input_2)\n",
    "vit_output_3 = TimeDistributed(vit_layer)(input_3)\n",
    "\n",
    "pooled_output_1 = tf.reduce_mean(vit_output_1, axis=2)\n",
    "pooled_output_2 = tf.reduce_mean(vit_output_2, axis=2)\n",
    "pooled_output_3 = tf.reduce_mean(vit_output_3, axis=2)\n",
    "\n",
    "merged = tf.concat([pooled_output_1, pooled_output_2, pooled_output_3], axis=-1)\n",
    "\n",
    "reshaped_merged = Reshape((-1, 64*3))(merged) \n",
    "lstm_out = LSTM(units=256, return_sequences=True, recurrent_activation='sigmoid')(reshaped_merged)\n",
    "lstm_out = LSTM(units=256, return_sequences=True, recurrent_activation='sigmoid')(lstm_out)\n",
    "lstm_out = Dropout(0.2)(lstm_out)\n",
    "\n",
    "dense_out = Dense(units=100)(lstm_out)\n",
    "dense_out = Dense(units=625)(dense_out)\n",
    "\n",
    "dense_out = Dropout(0.2)(dense_out)\n",
    "\n",
    "output = Activation('softmax')(dense_out)\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=output)\n",
    "\n",
    "def cancel_loss(y_true, y_pred):\n",
    "    y_true_truncated = y_true[:, 40:, :, :]\n",
    "    y_pred_truncated = y_pred[:, 40:, :, :]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true_truncated, y_pred_truncated)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=cancel_loss)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e74920b-3b4c-4889-adf2-4896e4739ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7688ac-8b1d-426c-9d0e-10cc0e0c7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(file_list, data_dir, batch_size):\n",
    "    total_files = len(file_list)\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(file_list)\n",
    "        for i in range(0, total_files, batch_size):\n",
    "            batch_files = file_list[i:i + batch_size]\n",
    "            feat_batch, army_batch, y_batch = [], [], []\n",
    "\n",
    "            for file in batch_files:\n",
    "                file_path = os.path.join(data_dir, file)\n",
    "                \n",
    "                data = np.load(file_path, allow_pickle=True)\n",
    "                \n",
    "                feat_batch.append(data['feat_padded'].astype(np.float32))\n",
    "                army_batch.append(data['army_padded'].astype(np.float32))\n",
    "                Y_padded = data['Y_padded'].astype(np.float32)\n",
    "                \n",
    "                # print(np.max(Y_padded[:,0]),np.max(Y_padded[:,1]))\n",
    "                y_dim1_one_hot = tf.keras.utils.to_categorical(Y_padded[:, 0], num_classes=25)\n",
    "                y_dim2_one_hot = tf.keras.utils.to_categorical(Y_padded[:, 1], num_classes=25)\n",
    "                y_one_hot = np.stack([y_dim1_one_hot, y_dim2_one_hot], axis=1)\n",
    "                y_batch.append(y_one_hot.astype(np.float32))\n",
    "            \n",
    "            feat_batch = np.array(feat_batch)\n",
    "            army_batch = np.array(army_batch)\n",
    "            y_batch = np.array(y_batch)\n",
    "            # print(feat_batch.shape)\n",
    "            yield (feat_batch, army_batch), y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0782cb3f-e9cc-41e1-b0a9-26212505bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dir = \"autodl-tmp/Numpy_Data\"\n",
    "file_names = os.listdir(data_dir)\n",
    "\n",
    "train_files, test_files = train_test_split(file_names, test_size=0.1, random_state=42)  # 80% train, 20% test\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.1, random_state=42)  # 75% train, 25% val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dfeb410-186b-4d79-9c9b-3f10fad97e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345 261 290\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files),len(val_files),len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2077832b-790d-4d53-9162-0d0b5794fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feat_padded', 'army_padded', 'Y_padded']\n",
      "(1000, 2)\n",
      "Max value in Y_padded[:, 1]: 5\n"
     ]
    }
   ],
   "source": [
    "file_path = '/root/autodl-tmp/Numpy_Data/data_row_16.npz'\n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "data_keys = data.files\n",
    "print(data_keys)\n",
    "# print(data['Y_padded'].shape)\n",
    "Y_padded = data['Y_padded']\n",
    "print(Y_padded.shape)\n",
    "print(\"Max value in Y_padded[:, 1]:\", np.max(Y_padded[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b211c50a-6880-49ec-b199-fec44f9253a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "# val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_files, data_dir, batch_size),\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32),  # feat_padded\n",
    "         tf.TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32)),  # army_padded\n",
    "        tf.TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32)  # Y_padded\n",
    "    )\n",
    ")\n",
    "# test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9455cdfc-06ae-4e2a-9feb-460e0ad9350e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 12:03:42.975300: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 23 of 100\n",
      "2024-09-21 12:03:52.970563: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 48 of 100\n",
      "2024-09-21 12:04:02.810704: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 71 of 100\n",
      "2024-09-21 12:04:12.856307: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 95 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_padded shape: (16, 1000, 25, 25)\n",
      "army_padded shape: (16, 1000, 25, 25)\n",
      "y_padded shape: (16, 1000, 2, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 12:04:14.755920: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "# for batch in train_dataset.take(1):  # Inspect 1 batch\n",
    "#     (feat_padded, army_padded), y_padded = batch\n",
    "#     print(\"feat_padded shape:\", feat_padded.shape)\n",
    "#     print(\"army_padded shape:\", army_padded.shape)\n",
    "#     print(\"y_padded shape:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f125af24-e50e-40b2-9525-0d0af9df8105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset element_spec=((TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32, name=None), TensorSpec(shape=(None, None, 25, 25), dtype=tf.float32, name=None)), TensorSpec(shape=(None, None, 2, 25), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53d304d1-cdaa-48c5-aee4-5704e791047e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 17:13:01.283258: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 49 of 100\n",
      "2024-09-21 17:13:11.215276: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 98 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/146 [..............................] - ETA: 49:42 - loss: 1.3527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 17:13:11.616350: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 68s 324ms/step - loss: 1.0509 - val_loss: 0.7465\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 45s 312ms/step - loss: 0.9968 - val_loss: 0.7326\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 46s 316ms/step - loss: 0.9726 - val_loss: 0.7538\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 46s 317ms/step - loss: 0.9315 - val_loss: 0.7708\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 45s 311ms/step - loss: 0.8759 - val_loss: 0.7785\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 46s 312ms/step - loss: 0.8394 - val_loss: 0.8292\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 45s 311ms/step - loss: 0.7618 - val_loss: 0.8989\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 47s 322ms/step - loss: 0.7146 - val_loss: 0.8925\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 47s 324ms/step - loss: 0.6475 - val_loss: 0.9582\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 48s 326ms/step - loss: 0.6135 - val_loss: 0.9752\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 47s 324ms/step - loss: 0.5561 - val_loss: 1.0355\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 47s 322ms/step - loss: 0.5385 - val_loss: 1.1017\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "total_files = len(os.listdir(\"autodl-tmp/Numpy_Data\"))\n",
    "train_steps = len(train_files) // batch_size\n",
    "validation_steps = len(val_files) // batch_size\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model.fit(train_dataset, \n",
    "          epochs=100, \n",
    "          steps_per_epoch=train_steps, \n",
    "          validation_data=val_dataset,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "model.save('model_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6094742d-1d4a-45ba-af0a-bfa52115ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/18 [===================>..........] - ETA: 1s - loss: 0.7503"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 17:23:19.110945: W tensorflow/core/framework/op_kernel.cc:1733] UNKNOWN: FileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1030, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/tmp/ipykernel_1228/1175695857.py\", line 17, in data_generator\n",
      "    data = np.load(file_path, allow_pickle=True)\n",
      "\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 390, in load\n",
      "    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n",
      "\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\nTraceback (most recent call last):\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1030, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/tmp/ipykernel_1228/1175695857.py\", line 17, in data_generator\n    data = np.load(file_path, allow_pickle=True)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 390, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n\nFileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\nTraceback (most recent call last):\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1030, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/tmp/ipykernel_1228/1175695857.py\", line 17, in data_generator\n    data = np.load(file_path, allow_pickle=True)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 390, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n\nFileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_7066]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_files) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\nTraceback (most recent call last):\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1030, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/tmp/ipykernel_1228/1175695857.py\", line 17, in data_generator\n    data = np.load(file_path, allow_pickle=True)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 390, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n\nFileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\nTraceback (most recent call last):\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1030, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/tmp/ipykernel_1228/1175695857.py\", line 17, in data_generator\n    data = np.load(file_path, allow_pickle=True)\n\n  File \"/root/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 390, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n\nFileNotFoundError: [Errno 2] No such file or directory: 'autodl-tmp/Numpy_Data/.data_row_3603.npz.zbLyAn'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_7066]"
     ]
    }
   ],
   "source": [
    "test_steps = len(test_files) // batch_size\n",
    "model.evaluate(test_dataset, steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d838f7d-d144-4a09-a366-d6c2c29fac95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [16, 16, 1000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m X1_train, X1_test, X2_train, X2_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeat_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marmy_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2646\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2650\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2651\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:453\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 453\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [16, 16, 1000]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(\n",
    "    feat_padded, army_padded, Y_padded, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fb420-00e1-4a8f-8b42-456aa19b2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "del feat_padded, army_padded, Y_padded\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a53f2a-e7a2-4f22-8cc7-5ab228a363f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.squeeze(y_train, axis=2)\n",
    "y_test = np.squeeze(y_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90137d9-b0c7-4ecd-9b93-13d9fe62070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 25\n",
    "\n",
    "y_train_class_1 = tf.one_hot(y_train[..., 0], depth=num_classes)\n",
    "y_train_class_2 = tf.one_hot(y_train[..., 1], depth=num_classes)\n",
    "y_train_categorical = tf.stack([y_train_class_1, y_train_class_2], axis=-2)\n",
    "\n",
    "y_test_class_1 = tf.one_hot(y_test[..., 0], depth=num_classes)\n",
    "y_test_class_2 = tf.one_hot(y_test[..., 1], depth=num_classes)\n",
    "y_test_categorical = tf.stack([y_test_class_1, y_test_class_2], axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1235c-b10e-42d5-a90d-5f2d8daaaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit([X1_train, X2_train], y_train_categorical, epochs=100, batch_size=16, verbose=1)\n",
    "model.save('model_full.h5')\n",
    "model.evaluate([X1_test, X2_test], y_test_categorical, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
